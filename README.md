Handwritten digit recognition has long been a favourite machine learning problem. Many popular datasets such as MNIST have been used extensively to train artificial
networks. In particular, deep convolutional neural networks have proven particularly adept at digit recognition. Since the convolution operation preserves the 2d spatial structure of the image, it is an especially logical choice for image classification.
As an example optimizing the hyperparameters of a CNN in order to improve the accuracy of handwritten digit recognition, this notebook is intended as a starting point for anyone lookign to optimize CNNs for more general image classification.
In order to test the performance of a simple CNN in comparison to multilayer perceptrons in image classification tasks, I trained 4 simple neural networks on the MNIST dataset
(segmented into training, testing and validation data with a total of 60000 training + validation and 10000 test samples). The 4 networks were
Model 1: MLP with a single hidden layer
Model 2: MLP with 2 hidden layers
Model 3: CNN with 1 convolutional layer
Model 4: CNN with 2 convolutional layers
Max pooling, flatten, dropout and fully connected layers were added to the CNNs to,
respectively
1) Reduce dimensionality
2) Convert the output of the final hidden layer to a 1d vector in order to pass it to the
softmax function of the output layer
3) To randomly exclude a small fraction of neurons to prevent overfitting
4) To pass outputs of the final hidden layer to the softmax function.
 I applied L2 regularisation to minimize overfitting. L2 is often preferred for image classification tasks because L2 regularization is better suited for dealing with correlated features, which are often present in image data.
L2 regularization encourages the weights to be small but does not set them to zero, allowing all of the input features to contribute to the model's predictions to some
degree.
I then varied some key hyperparameters (+/- Gaussian noise, ReLU vs sigmoid acivation function, number of epochs, +/- L2 regularization) of the networks and compared their
performances. Each data point can be generated by varying the random seed for a given set of hyperparameters.
