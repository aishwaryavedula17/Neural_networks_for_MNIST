{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoZypzmpy4dYauCCiWCbvq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishwaryavedula17/Neural_networks_for_MNIST/blob/main/Neural_networks_for_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI96wXs2PpYL"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, regularizers\n",
        "#always initialize weights and biases from the same point for\n",
        "reproducibility of results\n",
        "random.seed(1)\n",
        "np.random.seed(2)\n",
        "tf.random.set_seed(3)\n",
        "# Load the MNIST dataset and split\n",
        "mnist_data = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist_data.load_data()\n",
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train= x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
        "#convert target labels to categorical\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "#define hyperparameters\n",
        "num_epochs= 50\n",
        "batch_size= 128\n",
        "learning_rate= 0.001\n",
        "l2_lambda= 0.01\n",
        "#define the neural network models\n",
        "model1relu = keras.Sequential(\n",
        "[\n",
        "keras.layers.Flatten(input_shape=(28, 28)),\n",
        "keras.layers.Dense(128, activation='relu',\n",
        "kernel_regularizer= regularizers.l2(l2_lambda)),\n",
        "keras.layers.Dense(10, activation='softmax')\n",
        "]\n",
        ")\n",
        "model1sigmoid= keras.Sequential(\n",
        "[\n",
        "keras.layers.Flatten(input_shape=(28, 28)),\n",
        "keras.layers.Dense(128, activation='sigmoid',\n",
        "kernel_regularizer= regularizers.l2(l2_lambda)),\n",
        "keras.layers.Dense(10, activation='softmax')\n",
        "]\n",
        ")\n",
        "# Compile the model with optimizer, loss function, and\n",
        "evaluation metric\n",
        "model1relu.compile(optimizer=tf.optimizers.Adam(learning_rate=le\n",
        "arning_rate),\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "model1sigmoid.compile(optimizer=tf.optimizers.Adam(learning_rate\n",
        "=learning_rate),\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "# Train the model\n",
        "history1relu= model1relu.fit(x_train, y_train,\n",
        "epochs=num_epochs, batch_size=batch_size,\n",
        "validation_split= 0.2,verbose =0)\n",
        "history1sigmoid= model1sigmoid.fit(x_train, y_train,\n",
        "epochs=num_epochs, batch_size=batch_size,\n",
        "validation_split= 0.2,verbose =0)\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss_relu, test_acc_relu = model1relu.evaluate(x_test,\n",
        "y_test)\n",
        "test_loss_sigmoid, test_acc_sigmoid =\n",
        "model1sigmoid.evaluate(x_test, y_test)\n",
        "print ('Test accuracy_relu', test_acc_relu)\n",
        "print('Test accuracy_sigmoid', test_acc_sigmoid)\n",
        "#plotting graphs\n",
        "plt.plot(history1relu.history['accuracy'], color= 'red', label=\n",
        "'Train_ReLU')\n",
        "plt.plot(history1sigmoid.history['accuracy'], color= 'blue',\n",
        "label= 'Train_sigmoid')\n",
        "plt.plot(history1relu.history['val_accuracy'], color= 'orange',\n",
        "label= 'Test_ReLU')\n",
        "plt.plot(history1sigmoid.history['val_accuracy'], color=\n",
        "'green', label= 'Test_sigmoid')\n",
        "plt.title('Model 1(MLP with 1 HL and L2 regularization) ReLU vs\n",
        "Sigmoid Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "plt.plot(history1relu.history['loss'], color= 'red', label=\n",
        "'Train_ReLU')\n",
        "plt.plot(history1relu.history['val_loss'], color= 'orange',\n",
        "label= 'Test_ReLU' )\n",
        "plt.plot(history1sigmoid.history['loss'], color= 'blue', label=\n",
        "'Train_sigmoid' )\n",
        "plt.plot(history1sigmoid.history['val_loss'], color= 'green',\n",
        "label= 'Test_sigmoid' )\n",
        "plt.title('Model 1 (MLP with 1 HL and L2 regularization) ReLU vs\n",
        "Sigmoid Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend( loc='upper left')\n",
        "plt.show()\n",
        "# MLP with 2 hidden layers\n",
        "model2 = keras.Sequential(\n",
        "[\n",
        "keras.layers.Flatten(input_shape=(28, 28)),\n",
        "keras.layers.Dense(128, activation='relu',\n",
        "kernel_regularizer= regularizers.l2(l2_lambda)),\n",
        "keras.layers.Dense(128, activation='relu',\n",
        "kernel_regularizer= regularizers.l2(l2_lambda)),\n",
        "keras.layers.Dense(10, activation='softmax')\n",
        "]\n",
        ")\n",
        "# Compile the model with optimizer, loss function, and\n",
        "evaluation metric\n",
        "model2.compile(optimizer=tf.optimizers.Adam(learning_rate=learni\n",
        "ng_rate),\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "# Train the model\n",
        "history2 = model2.fit(x_train, y_train,\n",
        "epochs=num_epochs, batch_size=batch_size,\n",
        "validation_split= 0.2,verbose =0)\n",
        "# Evaluate the model on the test dataset\n",
        "test2_loss, test2_acc = model2.evaluate(x_test, y_test)\n",
        "print('Test2 accuracy:', test2_acc)\n",
        "# plot figures\n",
        "plt.plot(history1relu.history['accuracy'], color= 'red', label=\n",
        "'Train_1HL')\n",
        "plt.plot(history2.history['accuracy'], color= 'blue', label=\n",
        "'Train_2HL')\n",
        "plt.plot(history1relu.history['val_accuracy'], color= 'orange',\n",
        "label= 'Test_1HL')\n",
        "plt.plot(history2.history['val_accuracy'], color= 'green',\n",
        "label= 'Test_2HL')\n",
        "plt.title('MLP with 1 and 2 hidden ReLU layer(s) Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "plt.plot(history1relu.history['loss'], color= 'red', label=\n",
        "'Train_1HL')\n",
        "plt.plot(history1relu.history['val_loss'], color= 'orange',\n",
        "label= 'Test_1HL' )\n",
        "plt.plot(history2.history['loss'], color= 'blue', label=\n",
        "'Train_2HL' )\n",
        "plt.plot(history2.history['val_loss'], color= 'green', label=\n",
        "'Test_2HL' )\n",
        "plt.title('MLP with 1 and 2 hidden ReLU layer(s) Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend( loc='upper left')\n",
        "plt.show()\n",
        "# model 3: CNN with 2 conv layers\n",
        "model3 = keras.Sequential(\n",
        "[\n",
        "keras.Input(shape= input_shape),\n",
        "keras.layers.Conv2D(32, kernel_size= (3,3), activation=\n",
        "'relu', kernel_regularizer= regularizers.l2(l2_lambda)),\n",
        "keras. layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "keras.layers.Conv2D(64, kernel_size=(3, 3),\n",
        "activation=\"relu\", kernel_regularizer=\n",
        "regularizers.l2(l2_lambda)),\n",
        "keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "keras.layers.Flatten(),\n",
        "keras.layers.Dropout(0.5),\n",
        "layers.Dense(num_classes, activation=\"softmax\")\n",
        "]\n",
        ")\n",
        "# Compile the model with optimizer, loss function, and\n",
        "evaluation metric\n",
        "model3.compile(optimizer=tf.optimizers.Adam(learning_rate=learni\n",
        "ng_rate),\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "# Train the model\n",
        "history3 = model3.fit(x_train, y_train,\n",
        "epochs=num_epochs, batch_size=batch_size,\n",
        "validation_data=(x_test, y_test),verbose =1)\n",
        "# Evaluate the model on the test dataset\n",
        "test3_loss, test3_acc = model3.evaluate(x_test, y_test)\n",
        "print('Test3 accuracy:', test3_acc)\n",
        "# model 4: CNN with 1 conv layer\n",
        "model4 = keras.Sequential(\n",
        "[\n",
        "keras.Input(shape= input_shape),\n",
        "keras.layers.Conv2D(64, kernel_size= (3,3), activation=\n",
        "'relu', kernel_regularizer= regularizers.l2(l2_lambda)),\n",
        "keras. layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "keras.layers.Flatten(),\n",
        "keras.layers.Dropout(0.5),\n",
        "layers.Dense(num_classes, activation=\"softmax\")\n",
        "]\n",
        ")\n",
        "# Compile the model with optimizer, loss function, and\n",
        "evaluation metric\n",
        "model4.compile(optimizer=tf.optimizers.Adam(learning_rate=learni\n",
        "ng_rate),\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "# Train the model\n",
        "history4 = model4.fit(x_train, y_train,\n",
        "epochs=num_epochs, batch_size=batch_size,\n",
        "validation_split= 0.2,verbose =1)\n",
        "# Evaluate the model on the test dataset\n",
        "test4_loss, test4_acc = model4.evaluate(x_test, y_test)\n",
        "print('Test4 accuracy:', test4_acc)\n",
        "# plot figures\n",
        "plt.plot(history3.history['accuracy'], color= 'red', label=\n",
        "'Train CNN_1')\n",
        "plt.plot(history3.history['val_accuracy'], color= 'orange',\n",
        "label= 'Test CNN_1')\n",
        "plt.plot(history4.history['accuracy'], color= 'blue', label=\n",
        "'Train CNN_2')\n",
        "plt.plot(history4.history['val_accuracy'], color= 'green',\n",
        "label= 'Test CNN_2')\n",
        "plt.title('CNN with 1 and 2 convolutional layer(s)- Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "plt.plot(history3.history['loss'], color= 'red', label= 'Train\n",
        "CNN_1')\n",
        "plt.plot(history3.history['val_loss'], color= 'orange', label=\n",
        "'Test CNN_1')\n",
        "plt.plot(history4.history['loss'], color= 'blue', label= 'Train\n",
        "CNN_2')\n",
        "plt.plot(history4.history['val_loss'], color= 'green', label=\n",
        "'Test CNN_2')\n",
        "plt.title('CNN with 1 and 2 convolutional layer(s)- Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ]
    }
  ]
}